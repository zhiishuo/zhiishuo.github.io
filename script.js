const i18n = {
  en: {
    nav_about: 'About', nav_research: 'Research', nav_news: 'News', nav_projects: 'Projects', nav_resume: 'Resume', nav_portfolio: 'Portfolio', nav_blog: 'Blog', nav_contact: 'Contact',
    hero_tag: "Hi, I'm", hero_subtitle: 'Ph.D. candidate in Computer Science, focusing on multimodal learning, affective computing, and robust speech understanding.',
    hero_meta_1: 'Multimodal Affective Computing', hero_meta_2: 'Audio-Visual Language Modeling', hero_meta_3: 'Robust Speech Intelligence',
    hero_btn_contact: 'Contact', hero_btn_projects: 'View Research',
    hero_role: 'Ph.D. Candidate Â· Sichuan University',
    hero_point_1: 'Research focus: multimodal sentiment and affective intelligence',
    hero_point_2: 'Methods: cross-modal alignment, robust representation learning',
    hero_point_3: 'Open to academic collaboration and applied research transfer',
    about_title: 'About',
    about_body: 'I am a Ph.D. candidate at the School of Computer Science, Sichuan University. My research centers on multimodal learning, affective computing, and robust speech understanding under real-world conditions. I work on multimodal sentiment analysis, cross-modal contrastive optimization, and noise-resilient speech recognition, with an emphasis on interpretability and generalization.',
    research_title: 'Research Focus',
    r1_title: 'Multimodal Emotion Understanding', r1_body: 'Modeling emotion from speech, vision, and text with depth-aware representations.',
    r2_title: 'LLM + MoE Systems', r2_body: 'Task-adaptive routing and efficient expert collaboration for better generalization.',
    r3_title: 'Applied Research Systems', r3_body: 'Bridging method development with reproducible deployment and evaluation workflows.',
    projects_title: 'Selected Projects', projects_note: 'Representative directions across multimodal learning and system implementation.',
    p1_body: 'Hierarchical emotion modeling with adaptive multi-level mixture-of-experts.',
    p2_body: 'End-to-end pipeline for multimodal affect analysis and conversational intelligence.',
    p3_body: 'Research workflow automation for experiment iteration, evaluation, and reporting.',
    p4_body: 'Toolchain for transforming research prototypes into reproducible demos.',
    news_title: 'News', news_note: 'Latest paper and research updates.',
    news_1: 'ðŸŽ‰ Our paper "AV-RISE: Hierarchical Cross-Modal Denoising for Learning Robust Audio-Visual Speech Representation" is accepted at ACM MM 2025.',
    news_2: 'ðŸ“– Published "WinNet: Make Only One Convolutional Layer Effective for Time Series Forecasting" in ICIC 2025.',
    news_3: 'ðŸŽ‰ Our paper "AMG-AVSR: Adaptive Modality Guidance for Audio-Visual Speech Recognition via Progressive Feature Enhancement" is accepted at ACML 2024.',
    resume_title: 'Resume', resume_btn: 'Repository',
    resume_body: 'For full curriculum vitae, publication list, and academic timeline, please contact me via email or GitHub.',
    portfolio_title: 'Portfolio',
    portfolio_body: 'Selected representative work is listed in the Projects section, including research systems and implementation artifacts.',
    blog_title: 'Blog',
    blog_body: 'This section hosts research notes, engineering logs, and project retrospectives.',
    contact_title: 'Contact', contact_chip: 'Available for collaboration',
    contact_body: 'Open to collaboration, research exchange, and product building.',
    contact_email_label: 'Email',
    contact_response_label: 'Typical response', contact_response_value: 'Usually within 24â€“48 hours.',
    footer: 'Built with clarity and curiosity.'
  },
  zh: {
    nav_about: 'å…³äºŽ', nav_research: 'ç ”ç©¶æ–¹å‘', nav_news: 'æ–°é—»', nav_projects: 'é¡¹ç›®', nav_resume: 'ç®€åŽ†', nav_portfolio: 'ä½œå“é›†', nav_blog: 'åšå®¢', nav_contact: 'è”ç³»',
    hero_tag: 'ä½ å¥½ï¼Œæˆ‘æ˜¯', hero_subtitle: 'å››å·å¤§å­¦è®¡ç®—æœºå­¦é™¢åšå£«ç ”ç©¶ç”Ÿï¼Œç ”ç©¶æ–¹å‘ä¸ºå¤šæ¨¡æ€å­¦ä¹ ã€æƒ…æ„Ÿè®¡ç®—ä¸Žé²æ£’è¯­éŸ³ç†è§£ã€‚',
    hero_meta_1: 'å¤šæ¨¡æ€æƒ…æ„Ÿè®¡ç®—', hero_meta_2: 'éŸ³è§†é¢‘è¯­è¨€å»ºæ¨¡', hero_meta_3: 'é²æ£’è¯­éŸ³æ™ºèƒ½',
    hero_btn_contact: 'è”ç³»æˆ‘', hero_btn_projects: 'æŸ¥çœ‹ç ”ç©¶',
    hero_role: 'å››å·å¤§å­¦ Â· è®¡ç®—æœºå­¦é™¢åšå£«ç ”ç©¶ç”Ÿ',
    hero_point_1: 'ç ”ç©¶é‡ç‚¹ï¼šå¤šæ¨¡æ€æƒ…æ„Ÿç†è§£ä¸Žæƒ…ç»ªæ™ºèƒ½',
    hero_point_2: 'æ–¹æ³•æ–¹å‘ï¼šè·¨æ¨¡æ€å¯¹é½ä¸Žé²æ£’è¡¨ç¤ºå­¦ä¹ ',
    hero_point_3: 'æ¬¢è¿Žå­¦æœ¯åˆä½œä¸Žç ”ç©¶æˆæžœè½¬åŒ–äº¤æµ',
    about_title: 'å…³äºŽæˆ‘',
    about_body: 'æˆ‘ç›®å‰åœ¨å››å·å¤§å­¦è®¡ç®—æœºå­¦é™¢æ”»è¯»åšå£«å­¦ä½ã€‚ç ”ç©¶èšç„¦äºŽçœŸå®žåœºæ™¯ä¸‹çš„å¤šæ¨¡æ€å­¦ä¹ ã€æƒ…æ„Ÿè®¡ç®—ä¸Žé²æ£’è¯­éŸ³ç†è§£ï¼Œå…·ä½“åŒ…æ‹¬å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æžã€è·¨æ¨¡æ€å¯¹æ¯”ä¼˜åŒ–ä»¥åŠæŠ—å™ªè¯­éŸ³è¯†åˆ«ï¼Œå…³æ³¨æ¨¡åž‹çš„å¯è§£é‡Šæ€§ä¸Žæ³›åŒ–èƒ½åŠ›ã€‚',
    research_title: 'ç ”ç©¶é‡ç‚¹',
    r1_title: 'å¤šæ¨¡æ€æƒ…æ„Ÿç†è§£', r1_body: 'ç”¨æ·±åº¦åˆ†å±‚è¡¨ç¤ºå»ºæ¨¡è¯­éŸ³ã€è§†è§‰ä¸Žæ–‡æœ¬ä¸­çš„æƒ…æ„Ÿä¿¡æ¯ã€‚',
    r2_title: 'LLM + MoE ç³»ç»Ÿ', r2_body: 'é€šè¿‡ä»»åŠ¡è‡ªé€‚åº”è·¯ç”±å’Œä¸“å®¶ååŒæå‡æ¨¡åž‹æ³›åŒ–èƒ½åŠ›ã€‚',
    r3_title: 'åº”ç”¨ç ”ç©¶ç³»ç»Ÿ', r3_body: 'é¢å‘å¯å¤çŽ°å®žéªŒä¸Žç¨³å¥éƒ¨ç½²çš„ç ”ç©¶ç³»ç»ŸåŒ–æ–¹æ³•ã€‚',
    projects_title: 'ä»£è¡¨æ€§é¡¹ç›®', projects_note: 'è¦†ç›–å¤šæ¨¡æ€å­¦ä¹ æ–¹æ³•ä¸Žç³»ç»Ÿå®žçŽ°çš„å…³é”®æ–¹å‘ã€‚',
    p1_body: 'åˆ†å±‚æƒ…æ„Ÿå»ºæ¨¡ä¸Žè‡ªé€‚åº”å¤šå±‚ä¸“å®¶æ··åˆæ¡†æž¶ã€‚',
    p2_body: 'å¤šæ¨¡æ€æƒ…æ„Ÿç†è§£ä¸Žå¯¹è¯æ™ºèƒ½çš„ä¸€ä½“åŒ–ç ”ç©¶æµæ°´çº¿ã€‚',
    p3_body: 'é¢å‘å®žéªŒè¿­ä»£ã€è¯„ä¼°ä¸Žæ±‡æŠ¥çš„ç ”ç©¶å·¥ä½œæµè‡ªåŠ¨åŒ–ã€‚',
    p4_body: 'å°†ç ”ç©¶åŽŸåž‹è½¬åŒ–ä¸ºå¯å¤çŽ°æ¼”ç¤ºç³»ç»Ÿçš„å·¥å…·é“¾ã€‚',
    news_title: 'æ–°é—»', news_note: 'æœ€æ–°è®ºæ–‡ä¸Žç ”ç©¶åŠ¨æ€ã€‚',
    news_1: 'ðŸŽ‰ è®ºæ–‡ "AV-RISE: Hierarchical Cross-Modal Denoising for Learning Robust Audio-Visual Speech Representation" å·²è¢« ACM MM 2025 æŽ¥æ”¶ã€‚',
    news_2: 'ðŸ“– å‘è¡¨ "WinNet: Make Only One Convolutional Layer Effective for Time Series Forecasting"ï¼ˆICIC 2025ï¼‰ã€‚',
    news_3: 'ðŸŽ‰ è®ºæ–‡ "AMG-AVSR: Adaptive Modality Guidance for Audio-Visual Speech Recognition via Progressive Feature Enhancement" å·²è¢« ACML 2024 æŽ¥æ”¶ã€‚',
    resume_title: 'ç®€åŽ†', resume_btn: 'é¡¹ç›®ä»“åº“',
    resume_body: 'å®Œæ•´ç®€åŽ†ã€è®ºæ–‡åˆ—è¡¨ä¸Žå­¦æœ¯ç»åŽ†æ—¶é—´çº¿å¯é€šè¿‡é‚®ç®±æˆ– GitHub èŽ·å–ã€‚',
    portfolio_title: 'ä½œå“é›†',
    portfolio_body: 'ä»£è¡¨æ€§å·¥ä½œå·²åœ¨ Projects å±•ç¤ºï¼Œæ¶µç›–ç ”ç©¶ç³»ç»Ÿä¸Žå·¥ç¨‹å®žçŽ°ã€‚',
    blog_title: 'åšå®¢',
    blog_body: 'è¯¥éƒ¨åˆ†ç”¨äºŽæŒç»­å‘å¸ƒç ”ç©¶ç¬”è®°ã€å·¥ç¨‹æ—¥å¿—ä¸Žé¡¹ç›®å¤ç›˜ã€‚',
    contact_title: 'è”ç³»æ–¹å¼', contact_chip: 'å¯åˆä½œçŠ¶æ€',
    contact_body: 'æ¬¢è¿Žäº¤æµåˆä½œã€ç ”ç©¶è®¨è®ºä¸Žäº§å“å…±å»ºã€‚',
    contact_email_label: 'é‚®ç®±',
    contact_response_label: 'å“åº”æ—¶é—´', contact_response_value: 'é€šå¸¸ 24â€“48 å°æ—¶å†…å›žå¤ã€‚',
    footer: 'ä»¥æ¸…æ™°ä¸Žå¥½å¥‡æž„å»ºã€‚'
  }
};

const html = document.documentElement;
const langToggle = document.getElementById('langToggle');
const year = document.getElementById('year');
const menuToggle = document.getElementById('menuToggle');
const primaryNav = document.getElementById('primaryNav');
const navLinks = [...document.querySelectorAll('.primary-nav a')];
const toTop = document.getElementById('toTop');
const revealItems = [...document.querySelectorAll('.reveal')];
const reduceMotion = window.matchMedia('(prefers-reduced-motion: reduce)').matches;

let lang = localStorage.getItem('lang') || 'en';
let ticking = false;

function renderLanguage() {
  html.lang = lang;
  document.querySelectorAll('[data-i18n]').forEach((el) => {
    const key = el.getAttribute('data-i18n');
    if (i18n[lang] && i18n[lang][key]) {
      el.textContent = i18n[lang][key];
    }
  });
  langToggle.textContent = lang === 'en' ? 'ä¸­æ–‡' : 'EN';
  localStorage.setItem('lang', lang);
}

function closeMenu() {
  primaryNav.classList.remove('open');
  menuToggle.setAttribute('aria-expanded', 'false');
}

langToggle.addEventListener('click', () => {
  lang = lang === 'en' ? 'zh' : 'en';
  renderLanguage();
});

menuToggle.addEventListener('click', () => {
  const willOpen = !primaryNav.classList.contains('open');
  primaryNav.classList.toggle('open', willOpen);
  menuToggle.setAttribute('aria-expanded', String(willOpen));
});

navLinks.forEach((link) => {
  link.addEventListener('click', () => {
    if (window.matchMedia('(max-width: 860px)').matches) closeMenu();
  });
});

window.addEventListener('resize', () => {
  if (!window.matchMedia('(max-width: 860px)').matches) closeMenu();
});

const sections = navLinks
  .map((link) => document.querySelector(link.getAttribute('href')))
  .filter(Boolean);

const sectionObserver = new IntersectionObserver(
  (entries) => {
    entries.forEach((entry) => {
      if (!entry.isIntersecting) return;
      navLinks.forEach((link) => {
        const active = link.getAttribute('href') === `#${entry.target.id}`;
        link.classList.toggle('active', active);
      });
    });
  },
  { threshold: 0.35, rootMargin: '-20% 0px -45% 0px' }
);
sections.forEach((s) => sectionObserver.observe(s));

if (reduceMotion) {
  revealItems.forEach((item) => item.classList.add('in'));
} else {
  const revealObserver = new IntersectionObserver(
    (entries, observer) => {
      entries.forEach((entry) => {
        if (entry.isIntersecting) {
          entry.target.classList.add('in');
          observer.unobserve(entry.target);
        }
      });
    },
    { threshold: 0.12 }
  );
  revealItems.forEach((item) => revealObserver.observe(item));
}

function handleScroll() {
  const show = window.scrollY > 420;
  toTop.classList.toggle('show', show);
  ticking = false;
}

window.addEventListener('scroll', () => {
  if (!ticking) {
    window.requestAnimationFrame(handleScroll);
    ticking = true;
  }
});

toTop.addEventListener('click', () => {
  window.scrollTo({ top: 0, behavior: reduceMotion ? 'auto' : 'smooth' });
});

year.textContent = String(new Date().getFullYear());
renderLanguage();
