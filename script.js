const i18n = {
  en: {
    nav_about: 'About', nav_research: 'Research', nav_news: 'News', nav_projects: 'Projects', nav_resume: 'Resume', nav_portfolio: 'Portfolio', nav_blog: 'Blog', nav_contact: 'Contact',
    hero_tag: "Hi, I'm", hero_subtitle: 'Multimodal AI researcher focused on emotion understanding, LLM systems, and practical intelligent products.',
    hero_meta_1: 'Multimodal Affective Computing', hero_meta_2: 'LLM + MoE Systems', hero_meta_3: 'Applied AI Productization',
    hero_btn_contact: 'Work with me', hero_btn_projects: 'View projects',
    hero_role: 'AI Researcher & Builder',
    hero_point_1: 'Research-to-product execution mindset',
    hero_point_2: 'Strong in language-vision-audio systems',
    hero_point_3: 'Open to research and product collaborations',
    about_title: 'About',
    about_body: 'I build intelligent systems that connect language, vision, and audio. My interests include multimodal affective computing, robust routing for experts, and turning research prototypes into products that people can actually use.',
    research_title: 'Research Focus',
    r1_title: 'Multimodal Emotion Understanding', r1_body: 'Modeling emotion from speech, vision, and text with depth-aware representations.',
    r2_title: 'LLM + MoE Systems', r2_body: 'Task-adaptive routing and efficient expert collaboration for better generalization.',
    r3_title: 'Applied AI Products', r3_body: 'Bridging research and deployment through reliable workflows and automation.',
    projects_title: 'Selected Projects', projects_note: 'Representative directions across research and productization.',
    p1_body: 'Hierarchical emotion modeling with adaptive multi-level mixture-of-experts.',
    p2_body: 'End-to-end pipeline for multimodal emotion analysis and conversational AI.',
    p3_body: 'Personal automation workflows for research, coding, and assistant orchestration.',
    p4_body: 'Templates and scripts to accelerate turning academic ideas into usable demos.',
    news_title: 'News', news_note: 'Latest paper and research updates.',
    news_1: 'ðŸŽ‰ Our paper "AV-RISE: Hierarchical Cross-Modal Denoising for Learning Robust Audio-Visual Speech Representation" is accepted at ACM MM 2025.',
    news_2: 'ðŸ“– Published "WinNet: Make Only One Convolutional Layer Effective for Time Series Forecasting" in ICIC 2025.',
    news_3: 'ðŸŽ‰ Our paper "AMG-AVSR: Adaptive Modality Guidance for Audio-Visual Speech Recognition via Progressive Feature Enhancement" is accepted at ACML 2024.',
    resume_title: 'Resume', resume_btn: 'View source',
    resume_body: 'For full CV details, publication list, and timeline, please contact me by email or GitHub. (You can replace this block with your PDF CV link anytime.)',
    portfolio_title: 'Portfolio',
    portfolio_body: 'Selected works are shown above in Projects. This section is reserved for future demos, figures, and interactive showcases.',
    blog_title: 'Blog',
    blog_body: 'Blog posts are coming soon. You can use this section for research notes, engineering logs, and project retrospectives.',
    contact_title: 'Contact', contact_chip: 'Available for collaboration',
    contact_body: 'Open to collaboration, research exchange, and product building.',
    contact_email_label: 'Email',
    contact_response_label: 'Typical response', contact_response_value: 'Usually within 24â€“48 hours.',
    footer: 'Built with clarity and curiosity.'
  },
  zh: {
    nav_about: 'å…³äºŽ', nav_research: 'ç ”ç©¶æ–¹å‘', nav_news: 'æ–°é—»', nav_projects: 'é¡¹ç›®', nav_resume: 'ç®€åŽ†', nav_portfolio: 'ä½œå“é›†', nav_blog: 'åšå®¢', nav_contact: 'è”ç³»',
    hero_tag: 'ä½ å¥½ï¼Œæˆ‘æ˜¯', hero_subtitle: 'ä¸“æ³¨å¤šæ¨¡æ€æƒ…æ„Ÿç†è§£ã€LLMç³»ç»Ÿä¸Žå¯è½åœ°æ™ºèƒ½äº§å“çš„ç ”ç©¶è€…ã€‚',
    hero_meta_1: 'å¤šæ¨¡æ€æƒ…æ„Ÿè®¡ç®—', hero_meta_2: 'LLM + MoE ç³»ç»Ÿ', hero_meta_3: 'AI äº§å“åŒ–è½åœ°',
    hero_btn_contact: 'è”ç³»åˆä½œ', hero_btn_projects: 'æŸ¥çœ‹é¡¹ç›®',
    hero_role: 'AI ç ”ç©¶è€…ä¸Žæž„å»ºè€…',
    hero_point_1: 'å…·å¤‡ä»Žç ”ç©¶åˆ°äº§å“çš„æ‰§è¡Œèƒ½åŠ›',
    hero_point_2: 'æ“…é•¿è¯­è¨€-è§†è§‰-éŸ³é¢‘æ™ºèƒ½ç³»ç»Ÿ',
    hero_point_3: 'æ¬¢è¿Žç ”ç©¶åˆä½œä¸Žäº§å“å…±å»º',
    about_title: 'å…³äºŽæˆ‘',
    about_body: 'æˆ‘ä¸»è¦åšè¿žæŽ¥è¯­è¨€ã€è§†è§‰å’ŒéŸ³é¢‘çš„æ™ºèƒ½ç³»ç»Ÿã€‚å…´è¶£é›†ä¸­åœ¨å¤šæ¨¡æ€æƒ…æ„Ÿè®¡ç®—ã€ä¸“å®¶è·¯ç”±æœºåˆ¶ï¼Œä»¥åŠæŠŠç ”ç©¶åŽŸåž‹å¿«é€Ÿäº§å“åŒ–ã€‚',
    research_title: 'ç ”ç©¶é‡ç‚¹',
    r1_title: 'å¤šæ¨¡æ€æƒ…æ„Ÿç†è§£', r1_body: 'ç”¨æ·±åº¦åˆ†å±‚è¡¨ç¤ºå»ºæ¨¡è¯­éŸ³ã€è§†è§‰ä¸Žæ–‡æœ¬ä¸­çš„æƒ…æ„Ÿä¿¡æ¯ã€‚',
    r2_title: 'LLM + MoE ç³»ç»Ÿ', r2_body: 'é€šè¿‡ä»»åŠ¡è‡ªé€‚åº”è·¯ç”±å’Œä¸“å®¶ååŒæå‡æ¨¡åž‹æ³›åŒ–èƒ½åŠ›ã€‚',
    r3_title: 'AIäº§å“åŒ–è½åœ°', r3_body: 'å°†ç ”ç©¶æˆæžœè½¬åŒ–ä¸ºç¨³å®šã€å¯ç”¨ã€å¯æŒç»­è¿­ä»£çš„å®žé™…äº§å“ã€‚',
    projects_title: 'ä»£è¡¨æ€§é¡¹ç›®', projects_note: 'è¦†ç›–ç ”ç©¶ä¸Žäº§å“åŒ–çš„æ ¸å¿ƒæ–¹å‘ã€‚',
    p1_body: 'åˆ†å±‚æƒ…æ„Ÿå»ºæ¨¡ä¸Žè‡ªé€‚åº”å¤šå±‚ä¸“å®¶æ··åˆæ¡†æž¶ã€‚',
    p2_body: 'å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æžä¸Žå¯¹è¯ç³»ç»Ÿä¸€ä½“åŒ–æµæ°´çº¿ã€‚',
    p3_body: 'ç”¨äºŽç ”ç©¶ä¸Žå¼€å‘çš„ä¸ªäººè‡ªåŠ¨åŒ–å·¥ä½œæµç³»ç»Ÿã€‚',
    p4_body: 'ä»Žè®ºæ–‡åˆ°Demoçš„å¿«é€Ÿè½¬åŒ–æ¨¡æ¿ä¸Žè„šæœ¬å·¥å…·é›†ã€‚',
    news_title: 'æ–°é—»', news_note: 'æœ€æ–°è®ºæ–‡ä¸Žç ”ç©¶åŠ¨æ€ã€‚',
    news_1: 'ðŸŽ‰ è®ºæ–‡ "AV-RISE: Hierarchical Cross-Modal Denoising for Learning Robust Audio-Visual Speech Representation" å·²è¢« ACM MM 2025 æŽ¥æ”¶ã€‚',
    news_2: 'ðŸ“– å‘è¡¨ "WinNet: Make Only One Convolutional Layer Effective for Time Series Forecasting"ï¼ˆICIC 2025ï¼‰ã€‚',
    news_3: 'ðŸŽ‰ è®ºæ–‡ "AMG-AVSR: Adaptive Modality Guidance for Audio-Visual Speech Recognition via Progressive Feature Enhancement" å·²è¢« ACML 2024 æŽ¥æ”¶ã€‚',
    resume_title: 'ç®€åŽ†', resume_btn: 'æŸ¥çœ‹æºç ',
    resume_body: 'å®Œæ•´ç®€åŽ†ã€è®ºæ–‡åˆ—è¡¨ä¸Žç»åŽ†æ—¶é—´çº¿å¯é€šè¿‡é‚®ç®±æˆ– GitHub è”ç³»æˆ‘èŽ·å–ã€‚ï¼ˆä½ ä¹Ÿå¯ä»¥éšæ—¶æŠŠè¿™é‡Œæ›¿æ¢æˆ CV PDF é“¾æŽ¥ï¼‰',
    portfolio_title: 'ä½œå“é›†',
    portfolio_body: 'ä»£è¡¨æ€§ä½œå“å·²åœ¨ä¸Šæ–¹ Projects å±•ç¤ºã€‚æœ¬åŒºé¢„ç•™ç»™åŽç»­ Demoã€å¯è§†åŒ–å›¾ä¾‹ä¸Žäº¤äº’å±•ç¤ºã€‚',
    blog_title: 'åšå®¢',
    blog_body: 'åšå®¢å†…å®¹å³å°†æ›´æ–°ï¼Œä½ å¯ä»¥åœ¨è¿™é‡Œæ”¾ç ”ç©¶ç¬”è®°ã€å·¥ç¨‹è®°å½•ä¸Žé¡¹ç›®å¤ç›˜ã€‚',
    contact_title: 'è”ç³»æ–¹å¼', contact_chip: 'å¯åˆä½œçŠ¶æ€',
    contact_body: 'æ¬¢è¿Žäº¤æµåˆä½œã€ç ”ç©¶è®¨è®ºä¸Žäº§å“å…±å»ºã€‚',
    contact_email_label: 'é‚®ç®±',
    contact_response_label: 'å“åº”æ—¶é—´', contact_response_value: 'é€šå¸¸ 24â€“48 å°æ—¶å†…å›žå¤ã€‚',
    footer: 'ä»¥æ¸…æ™°ä¸Žå¥½å¥‡æž„å»ºã€‚'
  }
};

const html = document.documentElement;
const langToggle = document.getElementById('langToggle');
const year = document.getElementById('year');
const menuToggle = document.getElementById('menuToggle');
const primaryNav = document.getElementById('primaryNav');
const navLinks = [...document.querySelectorAll('.primary-nav a')];
const toTop = document.getElementById('toTop');
const revealItems = [...document.querySelectorAll('.reveal')];
const reduceMotion = window.matchMedia('(prefers-reduced-motion: reduce)').matches;

let lang = localStorage.getItem('lang') || 'en';
let ticking = false;

function renderLanguage() {
  html.lang = lang;
  document.querySelectorAll('[data-i18n]').forEach((el) => {
    const key = el.getAttribute('data-i18n');
    if (i18n[lang] && i18n[lang][key]) {
      el.textContent = i18n[lang][key];
    }
  });
  langToggle.textContent = lang === 'en' ? 'ä¸­æ–‡' : 'EN';
  localStorage.setItem('lang', lang);
}

function closeMenu() {
  primaryNav.classList.remove('open');
  menuToggle.setAttribute('aria-expanded', 'false');
}

langToggle.addEventListener('click', () => {
  lang = lang === 'en' ? 'zh' : 'en';
  renderLanguage();
});

menuToggle.addEventListener('click', () => {
  const willOpen = !primaryNav.classList.contains('open');
  primaryNav.classList.toggle('open', willOpen);
  menuToggle.setAttribute('aria-expanded', String(willOpen));
});

navLinks.forEach((link) => {
  link.addEventListener('click', () => {
    if (window.matchMedia('(max-width: 860px)').matches) closeMenu();
  });
});

window.addEventListener('resize', () => {
  if (!window.matchMedia('(max-width: 860px)').matches) closeMenu();
});

const sections = navLinks
  .map((link) => document.querySelector(link.getAttribute('href')))
  .filter(Boolean);

const sectionObserver = new IntersectionObserver(
  (entries) => {
    entries.forEach((entry) => {
      if (!entry.isIntersecting) return;
      navLinks.forEach((link) => {
        const active = link.getAttribute('href') === `#${entry.target.id}`;
        link.classList.toggle('active', active);
      });
    });
  },
  { threshold: 0.35, rootMargin: '-20% 0px -45% 0px' }
);
sections.forEach((s) => sectionObserver.observe(s));

if (reduceMotion) {
  revealItems.forEach((item) => item.classList.add('in'));
} else {
  const revealObserver = new IntersectionObserver(
    (entries, observer) => {
      entries.forEach((entry) => {
        if (entry.isIntersecting) {
          entry.target.classList.add('in');
          observer.unobserve(entry.target);
        }
      });
    },
    { threshold: 0.12 }
  );
  revealItems.forEach((item) => revealObserver.observe(item));
}

function handleScroll() {
  const show = window.scrollY > 420;
  toTop.classList.toggle('show', show);
  ticking = false;
}

window.addEventListener('scroll', () => {
  if (!ticking) {
    window.requestAnimationFrame(handleScroll);
    ticking = true;
  }
});

toTop.addEventListener('click', () => {
  window.scrollTo({ top: 0, behavior: reduceMotion ? 'auto' : 'smooth' });
});

year.textContent = String(new Date().getFullYear());
renderLanguage();
